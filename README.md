# Data-Science Projects using DL
![Neural_Networks_2880x1620-2880x1620-ezgif com-webp-to-jpg-converter](https://github.com/nelinazemi/Data-Science-DL/assets/113586221/0c867883-70cd-496f-8de1-82b7eb4828de)
#### Welcome to my repo containing data science projects on various topics, showcasing knowledge in Deep Learning and the ability to apply it to real-world complex models.

## Projects:
- ### 1- [Multi-Layer Perceptron (MLP) for Classification:](https://github.com/nelinazemi/Data-Science-DL/tree/f29a2b9145ae41fcc3aa59e89c7f866f4b3fa7a8/Multilayer%20Perceptron%20(MLP)%20Implementation/MLP%20for%20Classification)
    - This project focuses on predicting the price range of mobile phones based on various features using a neural network model.
    - A MLP model is defined with two hidden layers. The model is moved to a GPU if available.
    - The model is trained using Stochastic Gradient Descent (SGD) as the optimizer and Cross-Entropy Loss as the loss function.
    - The model's performance is evaluated on the validation set.survived or not based on the given features with a high degree of accuracy.
    - 
- ### 2- [Multi-Layer Perceptron (MLP) for Regression:](https://github.com/nelinazemi/Data-Science-DL/tree/f29a2b9145ae41fcc3aa59e89c7f866f4b3fa7a8/Multilayer%20Perceptron%20(MLP)%20Implementation/MLP%20for%20Regression)
  - This project focuses on predicting the median house value based on various features using a neural network model.
  - The model is trained using Stochastic Gradient Descent (SGD) as the optimizer and Mean Squared Error (MSE) as the loss function. The training process involves calculating the loss for each batch of data and updating the model parameters.
  - The model's predictions on the test set are compared with the actual values to compute the R-squared score.
  - 

- ### 3- [RNN-based Neural Networks:](https://github.com/nelinazemi/Data-Science-DL/tree/9e32698c3f5523395d28f4485011af1fd84d83ff/Recurrent%20Neural%20Network)

  - This folder contains three types of neural network models: LSTM (Long Short-Term Memory), RNN (Recurrent Neural Network), and TNN (Tensorial Neural Network). These models have been trained on the UCI-HAR (Human Activity Recognition Using Smartphones) dataset. The UCI-HAR dataset consists of recordings from the accelerometers and gyroscopes of smartphones worn by 30 participants while performing six different activities: walking, walking upstairs, walking downstairs, sitting, standing, and laying. The dataset includes a 561-feature vector with time and frequency domain variables1.
    
  - Neural Network Models:
    
        - LSTM (Long Short-Term Memory): LSTM networks are a type of RNN designed to handle long-term dependencies and mitigate the vanishing gradient problem. They are particularly effective for sequential data and time series prediction.
        - RNN (Recurrent Neural Network): RNNs are a class of neural networks that process sequential data by maintaining a hidden state that captures information from previous time steps. They are commonly used for tasks such as text, speech, and time series analysis.
        - TNN (Tensorial Neural Network): TNNs combine the principles of tensor networks and neural networks to handle high-dimensional data efficiently. They are used for tasks such as network compression, information fusion, and quantum circuit simulation.
