{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPS+EQ7dht97E5MhpJEzpJT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Google Drive**"],"metadata":{"id":"lhmaMb46nW70"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gR9GIPEYnP4b","executionInfo":{"status":"ok","timestamp":1730635122325,"user_tz":-210,"elapsed":82843,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"83f058ca-b85d-4834-f614-fe933a8348c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# **Installments**"],"metadata":{"id":"uh7T4DUwnj9u"}},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwFerfINnTKM","executionInfo":{"status":"ok","timestamp":1730635133802,"user_tz":-210,"elapsed":6872,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"007935cc-9afc-4925-cd92-281373fc6ed7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.5.1-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.0+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n","Downloading torchmetrics-1.5.1-py3-none-any.whl (890 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.6/890.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.11.8 torchmetrics-1.5.1\n"]}]},{"cell_type":"code","source":["!pip install torch==1.12.1 torchdata==0.4.1 torchtext==0.13.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGFNhVwHnrNl","executionInfo":{"status":"ok","timestamp":1730635230771,"user_tz":-210,"elapsed":96986,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"9ca7bcb2-f4da-4b76-e452-7310542774f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==1.12.1\n","  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n","Collecting torchdata==0.4.1\n","  Downloading torchdata-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n","Collecting torchtext==0.13.1\n","  Downloading torchtext-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.9 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1) (4.12.2)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.4.1) (2.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.4.1) (2.32.3)\n","Collecting portalocker>=2.0.0 (from torchdata==0.4.1)\n","  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.13.1) (4.66.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.13.1) (1.26.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.4.1) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.4.1) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.4.1) (2024.8.30)\n","Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchdata-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchtext-0.13.1-cp310-cp310-manylinux1_x86_64.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n","Installing collected packages: torch, portalocker, torchtext, torchdata\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.0+cu121\n","    Uninstalling torch-2.5.0+cu121:\n","      Successfully uninstalled torch-2.5.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","peft 0.13.2 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\n","torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 1.12.1 which is incompatible.\n","torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed portalocker-2.10.1 torch-1.12.1 torchdata-0.4.1 torchtext-0.13.1\n"]}]},{"cell_type":"markdown","source":["# **Imports**"],"metadata":{"id":"kDsai3T_n3tS"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, random_split\n","\n","import torchtext\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchmetrics import Accuracy\n","\n","from tqdm import tqdm"],"metadata":{"id":"hWAAEdhdn67A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(torch.__version__)\n","print(torchtext.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l2JQtrd6nwaF","executionInfo":{"status":"ok","timestamp":1730635236603,"user_tz":-210,"elapsed":29,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"18c0307d-8a05-400d-e82d-15f98cfd032e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.1+cu102\n","0.13.1\n"]}]},{"cell_type":"markdown","source":["# **Preprocessing**"],"metadata":{"id":"fOBJZQQArmJx"}},{"cell_type":"markdown","source":["In the following you'll see some preprocessing and transformation techniques used for natural language processing"],"metadata":{"id":"rlw7hJ48u22K"}},{"cell_type":"code","source":["from torchtext.data.utils import get_tokenizer"],"metadata":{"id":"iPHN6WMvrVKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = get_tokenizer('basic_english')"],"metadata":{"id":"9syvc-efzo2_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example1 = 'hello world!'\n","tokenized_example1 = tokenizer(example1)\n","tokenized_example1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pulBiLUro24","executionInfo":{"status":"ok","timestamp":1730635236605,"user_tz":-210,"elapsed":23,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"453d2b0f-0dba-48ab-aef5-72c0c42cc800"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hello', 'world', '!']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## **Vectorization**"],"metadata":{"id":"fKxBtlMjsLCE"}},{"cell_type":"code","source":["from torchtext.vocab import GloVe"],"metadata":{"id":"t5PBZT6Nr4Ju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = GloVe(name='6B', dim=50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzRbscoksNsZ","executionInfo":{"status":"ok","timestamp":1730635448253,"user_tz":-210,"elapsed":211666,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"df8c88fd-d741-4200-f43a-9147aead97c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.6B.zip: 862MB [02:42, 5.31MB/s]                           \n","100%|█████████▉| 399999/400000 [00:12<00:00, 31317.82it/s]\n"]}]},{"cell_type":"markdown","source":["We can both access the word by its index and its index to access the word"],"metadata":{"id":"4V40OpjEsTn2"}},{"cell_type":"code","source":["# getting the list of words in Glov dic\n","vocab.itos"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1BG84xksP6M","executionInfo":{"status":"ok","timestamp":1730635448254,"user_tz":-210,"elapsed":107,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"747316da-75e8-4339-eca5-4b265e4aee67"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the',\n"," ',',\n"," '.',\n"," 'of',\n"," 'to',\n"," 'and',\n"," 'in',\n"," 'a',\n"," '\"',\n"," \"'s\",\n"," 'for',\n"," '-',\n"," 'that',\n"," 'on',\n"," 'is',\n"," 'was',\n"," 'said',\n"," 'with',\n"," 'he',\n"," 'as',\n"," 'it',\n"," 'by',\n"," 'at',\n"," '(',\n"," ')',\n"," 'from',\n"," 'his',\n"," \"''\",\n"," '``',\n"," 'an',\n"," 'be',\n"," 'has',\n"," 'are',\n"," 'have',\n"," 'but',\n"," 'were',\n"," 'not',\n"," 'this',\n"," 'who',\n"," 'they',\n"," 'had',\n"," 'i',\n"," 'which',\n"," 'will',\n"," 'their',\n"," ':',\n"," 'or',\n"," 'its',\n"," 'one',\n"," 'after',\n"," 'new',\n"," 'been',\n"," 'also',\n"," 'we',\n"," 'would',\n"," 'two',\n"," 'more',\n"," \"'\",\n"," 'first',\n"," 'about',\n"," 'up',\n"," 'when',\n"," 'year',\n"," 'there',\n"," 'all',\n"," '--',\n"," 'out',\n"," 'she',\n"," 'other',\n"," 'people',\n"," \"n't\",\n"," 'her',\n"," 'percent',\n"," 'than',\n"," 'over',\n"," 'into',\n"," 'last',\n"," 'some',\n"," 'government',\n"," 'time',\n"," '$',\n"," 'you',\n"," 'years',\n"," 'if',\n"," 'no',\n"," 'world',\n"," 'can',\n"," 'three',\n"," 'do',\n"," ';',\n"," 'president',\n"," 'only',\n"," 'state',\n"," 'million',\n"," 'could',\n"," 'us',\n"," 'most',\n"," '_',\n"," 'against',\n"," 'u.s.',\n"," 'so',\n"," 'them',\n"," 'what',\n"," 'him',\n"," 'united',\n"," 'during',\n"," 'before',\n"," 'may',\n"," 'since',\n"," 'many',\n"," 'while',\n"," 'where',\n"," 'states',\n"," 'because',\n"," 'now',\n"," 'city',\n"," 'made',\n"," 'like',\n"," 'between',\n"," 'did',\n"," 'just',\n"," 'national',\n"," 'day',\n"," 'country',\n"," 'under',\n"," 'such',\n"," 'second',\n"," 'then',\n"," 'company',\n"," 'group',\n"," 'any',\n"," 'through',\n"," 'china',\n"," 'four',\n"," 'being',\n"," 'down',\n"," 'war',\n"," 'back',\n"," 'off',\n"," 'south',\n"," 'american',\n"," 'minister',\n"," 'police',\n"," 'well',\n"," 'including',\n"," 'team',\n"," 'international',\n"," 'week',\n"," 'officials',\n"," 'still',\n"," 'both',\n"," 'even',\n"," 'high',\n"," 'part',\n"," 'told',\n"," 'those',\n"," 'end',\n"," 'former',\n"," 'these',\n"," 'make',\n"," 'billion',\n"," 'work',\n"," 'our',\n"," 'home',\n"," 'school',\n"," 'party',\n"," 'house',\n"," 'old',\n"," 'later',\n"," 'get',\n"," 'another',\n"," 'tuesday',\n"," 'news',\n"," 'long',\n"," 'five',\n"," 'called',\n"," '1',\n"," 'wednesday',\n"," 'military',\n"," 'way',\n"," 'used',\n"," 'much',\n"," 'next',\n"," 'monday',\n"," 'thursday',\n"," 'friday',\n"," 'game',\n"," 'here',\n"," '?',\n"," 'should',\n"," 'take',\n"," 'very',\n"," 'my',\n"," 'north',\n"," 'security',\n"," 'season',\n"," 'york',\n"," 'how',\n"," 'public',\n"," 'early',\n"," 'according',\n"," 'several',\n"," 'court',\n"," 'say',\n"," 'around',\n"," 'foreign',\n"," '10',\n"," 'until',\n"," 'set',\n"," 'political',\n"," 'says',\n"," 'market',\n"," 'however',\n"," 'family',\n"," 'life',\n"," 'same',\n"," 'general',\n"," '–',\n"," 'left',\n"," 'good',\n"," 'top',\n"," 'university',\n"," 'going',\n"," 'number',\n"," 'major',\n"," 'known',\n"," 'points',\n"," 'won',\n"," 'six',\n"," 'month',\n"," 'dollars',\n"," 'bank',\n"," '2',\n"," 'iraq',\n"," 'use',\n"," 'members',\n"," 'each',\n"," 'area',\n"," 'found',\n"," 'official',\n"," 'sunday',\n"," 'place',\n"," 'go',\n"," 'based',\n"," 'among',\n"," 'third',\n"," 'times',\n"," 'took',\n"," 'right',\n"," 'days',\n"," 'local',\n"," 'economic',\n"," 'countries',\n"," 'see',\n"," 'best',\n"," 'report',\n"," 'killed',\n"," 'held',\n"," 'business',\n"," 'west',\n"," 'does',\n"," 'own',\n"," '%',\n"," 'came',\n"," 'law',\n"," 'months',\n"," 'women',\n"," \"'re\",\n"," 'power',\n"," 'think',\n"," 'service',\n"," 'children',\n"," 'bush',\n"," 'show',\n"," '/',\n"," 'help',\n"," 'chief',\n"," 'saturday',\n"," 'system',\n"," 'john',\n"," 'support',\n"," 'series',\n"," 'play',\n"," 'office',\n"," 'following',\n"," 'me',\n"," 'meeting',\n"," 'expected',\n"," 'late',\n"," 'washington',\n"," 'games',\n"," 'european',\n"," 'league',\n"," 'reported',\n"," 'final',\n"," 'added',\n"," 'without',\n"," 'british',\n"," 'white',\n"," 'history',\n"," 'man',\n"," 'men',\n"," 'became',\n"," 'want',\n"," 'march',\n"," 'case',\n"," 'few',\n"," 'run',\n"," 'money',\n"," 'began',\n"," 'open',\n"," 'name',\n"," 'trade',\n"," 'center',\n"," '3',\n"," 'israel',\n"," 'oil',\n"," 'too',\n"," 'al',\n"," 'film',\n"," 'win',\n"," 'led',\n"," 'east',\n"," 'central',\n"," '20',\n"," 'air',\n"," 'come',\n"," 'chinese',\n"," 'town',\n"," 'leader',\n"," 'army',\n"," 'line',\n"," 'never',\n"," 'little',\n"," 'played',\n"," 'prime',\n"," 'death',\n"," 'companies',\n"," 'least',\n"," 'put',\n"," 'forces',\n"," 'past',\n"," 'de',\n"," 'half',\n"," 'june',\n"," 'saying',\n"," 'know',\n"," 'federal',\n"," 'french',\n"," 'peace',\n"," 'earlier',\n"," 'capital',\n"," 'force',\n"," 'great',\n"," 'union',\n"," 'near',\n"," 'released',\n"," 'small',\n"," 'department',\n"," 'every',\n"," 'health',\n"," 'japan',\n"," 'head',\n"," 'ago',\n"," 'night',\n"," 'big',\n"," 'cup',\n"," 'election',\n"," 'region',\n"," 'director',\n"," 'talks',\n"," 'program',\n"," 'far',\n"," 'today',\n"," 'statement',\n"," 'july',\n"," 'although',\n"," 'district',\n"," 'again',\n"," 'born',\n"," 'development',\n"," 'leaders',\n"," 'council',\n"," 'close',\n"," 'record',\n"," 'along',\n"," 'county',\n"," 'france',\n"," 'went',\n"," 'point',\n"," 'must',\n"," 'spokesman',\n"," 'your',\n"," 'member',\n"," 'plan',\n"," 'financial',\n"," 'april',\n"," 'recent',\n"," 'campaign',\n"," 'become',\n"," 'troops',\n"," 'whether',\n"," 'lost',\n"," 'music',\n"," '15',\n"," 'got',\n"," 'israeli',\n"," '30',\n"," 'need',\n"," '4',\n"," 'lead',\n"," 'already',\n"," 'russia',\n"," 'though',\n"," 'might',\n"," 'free',\n"," 'hit',\n"," 'rights',\n"," '11',\n"," 'information',\n"," 'away',\n"," '12',\n"," '5',\n"," 'others',\n"," 'control',\n"," 'within',\n"," 'large',\n"," 'economy',\n"," 'press',\n"," 'agency',\n"," 'water',\n"," 'died',\n"," 'career',\n"," 'making',\n"," '...',\n"," 'deal',\n"," 'attack',\n"," 'side',\n"," 'seven',\n"," 'better',\n"," 'less',\n"," 'september',\n"," 'once',\n"," 'clinton',\n"," 'main',\n"," 'due',\n"," 'committee',\n"," 'building',\n"," 'conference',\n"," 'club',\n"," 'january',\n"," 'decision',\n"," 'stock',\n"," 'america',\n"," 'given',\n"," 'give',\n"," 'often',\n"," 'announced',\n"," 'television',\n"," 'industry',\n"," 'order',\n"," 'young',\n"," \"'ve\",\n"," 'palestinian',\n"," 'age',\n"," 'start',\n"," 'administration',\n"," 'russian',\n"," 'prices',\n"," 'round',\n"," 'december',\n"," 'nations',\n"," \"'m\",\n"," 'human',\n"," 'india',\n"," 'defense',\n"," 'asked',\n"," 'total',\n"," 'october',\n"," 'players',\n"," 'bill',\n"," 'important',\n"," 'southern',\n"," 'move',\n"," 'fire',\n"," 'population',\n"," 'rose',\n"," 'november',\n"," 'include',\n"," 'further',\n"," 'nuclear',\n"," 'street',\n"," 'taken',\n"," 'media',\n"," 'different',\n"," 'issue',\n"," 'received',\n"," 'secretary',\n"," 'return',\n"," 'college',\n"," 'working',\n"," 'community',\n"," 'eight',\n"," 'groups',\n"," 'despite',\n"," 'level',\n"," 'largest',\n"," 'whose',\n"," 'attacks',\n"," 'germany',\n"," 'august',\n"," 'change',\n"," 'church',\n"," 'nation',\n"," 'german',\n"," 'station',\n"," 'london',\n"," 'weeks',\n"," 'having',\n"," '18',\n"," 'research',\n"," 'black',\n"," 'services',\n"," 'story',\n"," '6',\n"," 'europe',\n"," 'sales',\n"," 'policy',\n"," 'visit',\n"," 'northern',\n"," 'lot',\n"," 'across',\n"," 'per',\n"," 'current',\n"," 'board',\n"," 'football',\n"," 'ministry',\n"," 'workers',\n"," 'vote',\n"," 'book',\n"," 'fell',\n"," 'seen',\n"," 'role',\n"," 'students',\n"," 'shares',\n"," 'iran',\n"," 'process',\n"," 'agreement',\n"," 'quarter',\n"," 'full',\n"," 'match',\n"," 'started',\n"," 'growth',\n"," 'yet',\n"," 'moved',\n"," 'possible',\n"," 'western',\n"," 'special',\n"," '100',\n"," 'plans',\n"," 'interest',\n"," 'behind',\n"," 'strong',\n"," 'england',\n"," 'named',\n"," 'food',\n"," 'period',\n"," 'real',\n"," 'authorities',\n"," 'car',\n"," 'term',\n"," 'rate',\n"," 'race',\n"," 'nearly',\n"," 'korea',\n"," 'enough',\n"," 'site',\n"," 'opposition',\n"," 'keep',\n"," '25',\n"," 'call',\n"," 'future',\n"," 'taking',\n"," 'island',\n"," '2008',\n"," '2006',\n"," 'road',\n"," 'outside',\n"," 'really',\n"," 'century',\n"," 'democratic',\n"," 'almost',\n"," 'single',\n"," 'share',\n"," 'leading',\n"," 'trying',\n"," 'find',\n"," 'album',\n"," 'senior',\n"," 'minutes',\n"," 'together',\n"," 'congress',\n"," 'index',\n"," 'australia',\n"," 'results',\n"," 'hard',\n"," 'hours',\n"," 'land',\n"," 'action',\n"," 'higher',\n"," 'field',\n"," 'cut',\n"," 'coach',\n"," 'elections',\n"," 'san',\n"," 'issues',\n"," 'executive',\n"," 'february',\n"," 'production',\n"," 'areas',\n"," 'river',\n"," 'face',\n"," 'using',\n"," 'japanese',\n"," 'province',\n"," 'park',\n"," 'price',\n"," 'commission',\n"," 'california',\n"," 'father',\n"," 'son',\n"," 'education',\n"," '7',\n"," 'village',\n"," 'energy',\n"," 'shot',\n"," 'short',\n"," 'africa',\n"," 'key',\n"," 'red',\n"," 'association',\n"," 'average',\n"," 'pay',\n"," 'exchange',\n"," 'eu',\n"," 'something',\n"," 'gave',\n"," 'likely',\n"," 'player',\n"," 'george',\n"," '2007',\n"," 'victory',\n"," '8',\n"," 'low',\n"," 'things',\n"," '2010',\n"," 'pakistan',\n"," '14',\n"," 'post',\n"," 'social',\n"," 'continue',\n"," 'ever',\n"," 'look',\n"," 'chairman',\n"," 'job',\n"," '2000',\n"," 'soldiers',\n"," 'able',\n"," 'parliament',\n"," 'front',\n"," 'himself',\n"," 'problems',\n"," 'private',\n"," 'lower',\n"," 'list',\n"," 'built',\n"," '13',\n"," 'efforts',\n"," 'dollar',\n"," 'miles',\n"," 'included',\n"," 'radio',\n"," 'live',\n"," 'form',\n"," 'david',\n"," 'african',\n"," 'increase',\n"," 'reports',\n"," 'sent',\n"," 'fourth',\n"," 'always',\n"," 'king',\n"," '50',\n"," 'tax',\n"," 'taiwan',\n"," 'britain',\n"," '16',\n"," 'playing',\n"," 'title',\n"," 'middle',\n"," 'meet',\n"," 'global',\n"," 'wife',\n"," '2009',\n"," 'position',\n"," 'located',\n"," 'clear',\n"," 'ahead',\n"," '2004',\n"," '2005',\n"," 'iraqi',\n"," 'english',\n"," 'result',\n"," 'release',\n"," 'violence',\n"," 'goal',\n"," 'project',\n"," 'closed',\n"," 'border',\n"," 'body',\n"," 'soon',\n"," 'crisis',\n"," 'division',\n"," '&amp;',\n"," 'served',\n"," 'tour',\n"," 'hospital',\n"," 'kong',\n"," 'test',\n"," 'hong',\n"," 'u.n.',\n"," 'inc.',\n"," 'technology',\n"," 'believe',\n"," 'organization',\n"," 'published',\n"," 'weapons',\n"," 'agreed',\n"," 'why',\n"," 'nine',\n"," 'summer',\n"," 'wanted',\n"," 'republican',\n"," 'act',\n"," 'recently',\n"," 'texas',\n"," 'course',\n"," 'problem',\n"," 'senate',\n"," 'medical',\n"," 'un',\n"," 'done',\n"," 'reached',\n"," 'star',\n"," 'continued',\n"," 'investors',\n"," 'living',\n"," 'care',\n"," 'signed',\n"," '17',\n"," 'art',\n"," 'provide',\n"," 'worked',\n"," 'presidential',\n"," 'gold',\n"," 'obama',\n"," 'morning',\n"," 'dead',\n"," 'opened',\n"," \"'ll\",\n"," 'event',\n"," 'previous',\n"," 'cost',\n"," 'instead',\n"," 'canada',\n"," 'band',\n"," 'teams',\n"," 'daily',\n"," '2001',\n"," 'available',\n"," 'drug',\n"," 'coming',\n"," '2003',\n"," 'investment',\n"," '’s',\n"," 'michael',\n"," 'civil',\n"," 'woman',\n"," 'training',\n"," 'appeared',\n"," '9',\n"," 'involved',\n"," 'indian',\n"," 'similar',\n"," 'situation',\n"," '24',\n"," 'los',\n"," 'running',\n"," 'fighting',\n"," 'mark',\n"," '40',\n"," 'trial',\n"," 'hold',\n"," 'australian',\n"," 'thought',\n"," '!',\n"," 'study',\n"," 'fall',\n"," 'mother',\n"," 'met',\n"," 'relations',\n"," 'anti',\n"," '2002',\n"," 'song',\n"," 'popular',\n"," 'base',\n"," 'tv',\n"," 'ground',\n"," 'markets',\n"," 'ii',\n"," 'newspaper',\n"," 'staff',\n"," 'saw',\n"," 'hand',\n"," 'hope',\n"," 'operations',\n"," 'pressure',\n"," 'americans',\n"," 'eastern',\n"," 'st.',\n"," 'legal',\n"," 'asia',\n"," 'budget',\n"," 'returned',\n"," 'considered',\n"," 'love',\n"," 'wrote',\n"," 'stop',\n"," 'fight',\n"," 'currently',\n"," 'charges',\n"," 'try',\n"," 'aid',\n"," 'ended',\n"," 'management',\n"," 'brought',\n"," 'cases',\n"," 'decided',\n"," 'failed',\n"," 'network',\n"," 'works',\n"," 'gas',\n"," 'turned',\n"," 'fact',\n"," 'vice',\n"," 'ca',\n"," 'mexico',\n"," 'trading',\n"," 'especially',\n"," 'reporters',\n"," 'afghanistan',\n"," 'common',\n"," 'looking',\n"," 'space',\n"," 'rates',\n"," 'manager',\n"," 'loss',\n"," '2011',\n"," 'justice',\n"," 'thousands',\n"," 'james',\n"," 'rather',\n"," 'fund',\n"," 'thing',\n"," 'republic',\n"," 'opening',\n"," 'accused',\n"," 'winning',\n"," 'scored',\n"," 'championship',\n"," 'example',\n"," 'getting',\n"," 'biggest',\n"," 'performance',\n"," 'sports',\n"," '1998',\n"," 'let',\n"," 'allowed',\n"," 'schools',\n"," 'means',\n"," 'turn',\n"," 'leave',\n"," 'no.',\n"," 'robert',\n"," 'personal',\n"," 'stocks',\n"," 'showed',\n"," 'light',\n"," 'arrested',\n"," 'person',\n"," 'either',\n"," 'offer',\n"," 'majority',\n"," 'battle',\n"," '19',\n"," 'class',\n"," 'evidence',\n"," 'makes',\n"," 'society',\n"," 'products',\n"," 'regional',\n"," 'needed',\n"," 'stage',\n"," 'am',\n"," 'doing',\n"," 'families',\n"," 'construction',\n"," 'various',\n"," '1996',\n"," 'sold',\n"," 'independent',\n"," 'kind',\n"," 'airport',\n"," 'paul',\n"," 'judge',\n"," 'internet',\n"," 'movement',\n"," 'room',\n"," 'followed',\n"," 'original',\n"," 'angeles',\n"," 'italy',\n"," '`',\n"," 'data',\n"," 'comes',\n"," 'parties',\n"," 'nothing',\n"," 'sea',\n"," 'bring',\n"," '2012',\n"," 'annual',\n"," 'officer',\n"," 'beijing',\n"," 'present',\n"," 'remain',\n"," 'nato',\n"," '1999',\n"," '22',\n"," 'remains',\n"," 'allow',\n"," 'florida',\n"," 'computer',\n"," '21',\n"," 'contract',\n"," 'coast',\n"," 'created',\n"," 'demand',\n"," 'operation',\n"," 'events',\n"," 'islamic',\n"," 'beat',\n"," 'analysts',\n"," 'interview',\n"," 'helped',\n"," 'child',\n"," 'probably',\n"," 'spent',\n"," 'asian',\n"," 'effort',\n"," 'cooperation',\n"," 'shows',\n"," 'calls',\n"," 'investigation',\n"," 'lives',\n"," 'video',\n"," 'yen',\n"," 'runs',\n"," 'tried',\n"," 'bad',\n"," 'described',\n"," '1994',\n"," 'toward',\n"," 'written',\n"," 'throughout',\n"," 'established',\n"," 'mission',\n"," 'associated',\n"," 'buy',\n"," 'growing',\n"," 'green',\n"," 'forward',\n"," 'competition',\n"," 'poor',\n"," 'latest',\n"," 'banks',\n"," 'question',\n"," '1997',\n"," 'prison',\n"," 'feel',\n"," 'attention',\n"," ...]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["len(vocab.itos)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vT6P78Gdsgsl","executionInfo":{"status":"ok","timestamp":1730635448254,"user_tz":-210,"elapsed":93,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"0fb50d92-6e46-42be-8a73-4ebdae515a9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["400000"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqzoWRDvBPf5"},"outputs":[],"source":["vec_boy = vocab.get_vecs_by_tokens('boy')\n","vec_girl = vocab.get_vecs_by_tokens('girl')\n","vec_earth = vocab.get_vecs_by_tokens('earth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74,"status":"ok","timestamp":1730635448255,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"},"user_tz":-210},"id":"IXs1FQ0SBr-o","outputId":"9001f915-b96e-4678-fdff-437abdf64aba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.2822)"]},"metadata":{},"execution_count":14}],"source":["F.cosine_similarity(vec_boy, vec_girl, dim=0)\n","F.cosine_similarity(vec_girl, vec_earth, dim=0)"]},{"cell_type":"markdown","source":["## **Transforms**"],"metadata":{"id":"RgCqnhbktv0K"}},{"cell_type":"code","source":["from torchtext import transforms as T"],"metadata":{"id":"m6x3_Ht_tsA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VOCAB_FILE = \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\""],"metadata":{"id":"4Ho41wCRtzW0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = T.BERTTokenizer(vocab_path=VOCAB_FILE,\n","                            do_lower_case=True,\n","                            return_tokens=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8sVNsnbt5U6","executionInfo":{"status":"ok","timestamp":1730635448256,"user_tz":-210,"elapsed":67,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"df83e4bf-7450-48a1-8176-ef5009bea98c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 232k/232k [00:00<00:00, 553kB/s]\n"]}]},{"cell_type":"code","source":["example2 = 'HI! HOW IS IT GOING?'\n","tokenizer(example2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JxBP593auMi2","executionInfo":{"status":"ok","timestamp":1730635448256,"user_tz":-210,"elapsed":62,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"8d4cabd7-d50e-43d3-893f-69b1449020e1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hi', '!', 'how', 'is', 'it', 'going', '?']"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["Extracting the index of words"],"metadata":{"id":"1MTlFNCducuN"}},{"cell_type":"code","source":["example3 = 'I am using my computer.'\n","example4 = 'I am a programmer.'"],"metadata":{"id":"F1dc9OuOvBKX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example3_tokens = tokenizer(example3)\n","example4_tokens = tokenizer(example4)"],"metadata":{"id":"qsMsp2kovK-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example3_indices = [vocab.stoi[token] for token in example3_tokens]\n","example4_indices = [vocab.stoi[token] for token in example4_tokens]\n","token_ids = [example3_indices, example4_indices]\n","\n","token_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CO9UxE2vR2-","executionInfo":{"status":"ok","timestamp":1730635448257,"user_tz":-210,"elapsed":56,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"2731ef6a-db56-4099-a2da-adfa7eed3dc0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[41, 913, 622, 192, 951, 2], [41, 913, 7, 19226, 2]]"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Making both vecs an equal size for further processing"],"metadata":{"id":"kZEAZrPHwfe2"}},{"cell_type":"code","source":["resizing_ids = T.ToTensor(padding_value=0)\n","resizing_ids(token_ids)\n","# T.ToTensor(padding_value=0)(token_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXBi47qxv4lD","executionInfo":{"status":"ok","timestamp":1730635448258,"user_tz":-210,"elapsed":53,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"2da266ee-0576-48c2-c5b2-2c5853fb2fe3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   41,   913,   622,   192,   951,     2],\n","        [   41,   913,     7, 19226,     2,     0]])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["T.Truncate(max_seq_len=3)(example3_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5Ln5oOgw1Tn","executionInfo":{"status":"ok","timestamp":1730635448259,"user_tz":-210,"elapsed":48,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"b8cb07e5-ba5d-4a98-fb9e-869506b9b918"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i', 'am', 'using']"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["# **Dataset**"],"metadata":{"id":"JTBuW0_px0fq"}},{"cell_type":"code","source":["from torchtext import datasets"],"metadata":{"id":"nfula1n7x5lc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set, test_set = datasets.AG_NEWS('/content/', split=('train', 'test'))"],"metadata":{"id":"TWsCJTD3x6wv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.utils.rnn import pad_sequence"],"metadata":{"id":"SxbhWpC_ygtb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(train_set))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaWvUcBGyq1_","executionInfo":{"status":"ok","timestamp":1730635450080,"user_tz":-210,"elapsed":1860,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"8d962b63-94aa-42d6-a562-3063d6a85453"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3,\n"," \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["So based on the structure of our dataset, we extract the text and label using a custom function"],"metadata":{"id":"ZNkaGXnVy8io"}},{"cell_type":"code","source":["from torchtext.data.utils import get_tokenizer"],"metadata":{"id":"srqClXZSzrw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = get_tokenizer('basic_english')"],"metadata":{"id":"J07YvAU_zuoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate(batch):\n","  labels = torch.LongTensor([b[0] for b in batch]) - 1\n","  texts = [b[1] for b in batch]\n","  text_tokens = [tokenizer(text) for text in texts]\n","  token_vectors = [vocab.get_vecs_by_tokens(tokens) for tokens in text_tokens]\n","  token_vectors = pad_sequence(token_vectors)\n","\n","  return token_vectors, labels"],"metadata":{"id":"DEGAVFHLyzdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Applying the custom function on our dataset"],"metadata":{"id":"iQ2mjTKv0I5F"}},{"cell_type":"code","source":["train_loader = DataLoader(train_set, batch_size=128, shuffle=True, collate_fn=collate)\n","test_loader = DataLoader(test_set, batch_size=256, collate_fn=collate)"],"metadata":{"id":"d9WwPBhJz-PY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wNSgStMX0MVF","executionInfo":{"status":"ok","timestamp":1730636504217,"user_tz":-210,"elapsed":11,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"dc868e45-a56c-40a5-f80b-b7d5c17d6205"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7d19ce5aa080>"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["x, y = next(iter(train_loader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x7sfc3aI0PJU","executionInfo":{"status":"ok","timestamp":1730636505182,"user_tz":-210,"elapsed":11,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"568d14ea-fe96-4ead-e506-b0ca2e8f1db6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:248: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"]}]},{"cell_type":"code","source":["x, y = next(iter(test_loader))"],"metadata":{"id":"ONKsPqu-4-Wi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mbpEEA0g1-Be"},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6s889_UrqDT"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_JNWs7q0m1h"},"outputs":[],"source":["def num_params(model):\n","  nums = sum(p.numel() for p in model.parameters())/1e6\n","  return nums"]},{"cell_type":"markdown","metadata":{"id":"Ggb4EOeBuSYQ"},"source":["## Init"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K07nHNbTuUzE"},"outputs":[],"source":["num_cls = 4\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","source":["# **Model**"],"metadata":{"id":"Rq_kYddM8DZm"}},{"cell_type":"markdown","source":["This part demonstrates how to load a pre-trained RoBERTa model, modify its structure, and manipulate embeddings for downstream tasks, particularly classification."],"metadata":{"id":"ORcfnoosAOPj"}},{"cell_type":"markdown","source":["In neural networks, especially those used for natural language processing (NLP), **embedding layers** and **embedding bags** play a crucial role in transforming categorical inputs (like words) into dense numerical vectors that a model can understand and process.\n","\n","### 1. **What is an Embedding Layer?**\n","An **embedding layer** is a type of neural network layer that maps discrete, categorical values (like words or tokens) to continuous vector representations. Each unique categorical value (e.g., each word in a vocabulary) gets assigned a unique vector of fixed dimensions. These vectors are learned during training, capturing semantic relationships between categories.\n","\n","For example, words with similar meanings (like \"king\" and \"queen\") will tend to have embeddings that are close in the vector space, as the embedding layer learns to encode such relationships based on the task it is trained on.\n","\n","#### Example of an Embedding Layer:\n","Suppose we have a vocabulary of 5 words: `[\"cat\", \"dog\", \"fish\", \"lion\", \"tiger\"]`, and we want each word to be represented by a vector of 3 dimensions.\n","\n","1. We assign an index to each word: `cat=0, dog=1, fish=2, lion=3, tiger=4`.\n","2. We initialize an embedding matrix of shape (5, 3). Each row of this matrix is a 3-dimensional vector representing one word:\n","   ```\n","   [[0.1, 0.2, 0.3],   # embedding for \"cat\"\n","    [0.4, 0.5, 0.6],   # embedding for \"dog\"\n","    [0.7, 0.8, 0.9],   # embedding for \"fish\"\n","    [0.2, 0.3, 0.4],   # embedding for \"lion\"\n","    [0.5, 0.6, 0.7]]   # embedding for \"tiger\"\n","   ```\n","3. When we input a word (like \"dog\") into the embedding layer, it returns the corresponding vector `[0.4, 0.5, 0.6]`.\n","\n","### 2. **What is an Embedding Bag?**\n","An **embedding bag** is an extension of the embedding layer that is designed to handle variable-length sequences and perform operations like averaging or summing over embeddings within a \"bag\" or a group of tokens. Instead of getting an embedding for each individual token, an embedding bag takes a sequence of token indices and computes a single embedding vector for the entire sequence by applying a reduction operation (e.g., sum or average).\n","\n","This is particularly useful for tasks like sentence classification, where we may want to represent a sentence with a single vector derived from its constituent word embeddings.\n","\n","#### Example of an Embedding Bag:\n","Suppose we have a sentence represented by the token indices `[0, 1, 3]` (corresponding to \"cat,\" \"dog,\" and \"lion\").\n","\n","1. The embedding bag takes these indices and looks up each token's vector:\n","   ```\n","   \"cat\" -> [0.1, 0.2, 0.3]\n","   \"dog\" -> [0.4, 0.5, 0.6]\n","   \"lion\" -> [0.2, 0.3, 0.4]\n","   ```\n","2. The embedding bag then combines these vectors, for example by summing:\n","   ```\n","   [0.1 + 0.4 + 0.2, 0.2 + 0.5 + 0.3, 0.3 + 0.6 + 0.4]\n","   = [0.7, 1.0, 1.3]\n","   ```\n","3. This summed vector `[0.7, 1.0, 1.3]` now represents the entire sentence.\n","\n","\n","### 3. **How Embedding Works: Step-by-Step Example**\n","\n","Imagine we have the following input sentence and embedding configuration:\n","- **Sentence**: \"I love dogs\"\n","- **Vocabulary**: `{\"I\": 0, \"love\": 1, \"dogs\": 2}`\n","- **Embedding dimension**: 2\n","\n","1. **Initialize an Embedding Matrix** of size `(vocabulary_size, embedding_dimension)`:\n","   ```\n","   [[0.2, 0.8],    # \"I\" embedding\n","    [0.6, 0.1],    # \"love\" embedding\n","    [0.3, 0.7]]    # \"dogs\" embedding\n","   ```\n","   \n","2. **Convert Words to Indices**:\n","   ```\n","   \"I\" -> 0\n","   \"love\" -> 1\n","   \"dogs\" -> 2\n","   ```\n","   Our sentence becomes: `[0, 1, 2]`.\n","\n","3. **Lookup the Embedding Vectors for Each Token**:\n","   ```\n","   [0.2, 0.8],  # \"I\"\n","   [0.6, 0.1],  # \"love\"\n","   [0.3, 0.7]   # \"dogs\"\n","   ```\n","\n","4. **Combine the Embeddings**:\n","   For individual embeddings, we leave them as-is. If we use an embedding bag with \"sum\" mode, we’d get:\n","   ```\n","   [0.2 + 0.6 + 0.3, 0.8 + 0.1 + 0.7] = [1.1, 1.6]\n","   ```\n","   This `[1.1, 1.6]` vector can now represent the sentence as a whole in tasks like classification."],"metadata":{"id":"eGx4bf9KCihk"}},{"cell_type":"code","source":["from torchtext import models\n","from torchtext.functional import to_tensor"],"metadata":{"id":"Pxms6HndARQa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["roberta_base = models.ROBERTA_BASE_ENCODER\n","roberta_base._head = nn.LazyLinear(20)\n","roberta_base._head = nn.Identity()"],"metadata":{"id":"cKXTPRrKAX5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_batch = [\"Hello world\", \"How are you!\"]"],"metadata":{"id":"jUlJD8xeAl87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["roberta_encoder = roberta_base.get_model()\n","roberta_transformer = roberta_base.transform()"],"metadata":{"id":"_ZAIgp4TAmmd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_tensor = to_tensor(roberta_transformer(input_batch), padding_value=1)"],"metadata":{"id":"mpLzZSEZA0MY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["roberta_embedded_weights = roberta_encoder.encoder.transformer.token_embedding.weight\n","torch.save(roberta_embedded_weights, 'roberta_embedded_weights.pt')\n","loaded_roberta_embedded_weights = torch.load('loaded_roberta_embedded_weights.pt')\n","\n","n, d = loaded_roberta_embedded_weights.shape\n","\n","embedding = nn.Embedding(n, d)\n","embedding = embedding.from_pretrained(loaded_roberta_embedded_weights)"],"metadata":{"id":"DWXqYZ28A-uv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchtext import models\n","from torchtext.functional import to_tensor"],"metadata":{"id":"eahJ2lVz5KOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RNNModel(nn.Module):\n","  def __init__(self, RNN, input_size, hidden_size, num_layers, bidirectional, num_cls):\n","    super().__init__()\n","    self.rnn = RNN(input_size=input_size,\n","                      hidden_size=hidden_size,\n","                      num_layers=num_layers,\n","                      bidirectional=bidirectional,\n","                      batch_first=False)\n","    self.fc = nn.LazyLinear(num_cls)\n","\n","  def forward(self, x):\n","    outputs, _ = self.rnn(x)\n","    y = self.fc(outputs)\n","    y = y.mean(dim=0)\n","    return y"],"metadata":{"id":"dwGnJRVZ5SVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RNNModel(nn.LSTM, 50, 128, 1, False, num_cls)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEU7NncT7sud","executionInfo":{"status":"ok","timestamp":1730635956427,"user_tz":-210,"elapsed":26,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"66c05f8c-0e09-48ac-9ba1-357cb52399d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]}]},{"cell_type":"markdown","metadata":{"id":"B_LljZFVAFPA"},"source":["## Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4W9MVeEqAYiq"},"outputs":[],"source":["def train_one_epoch(model, train_loader, loss_fn, optimizer, epoch=None):\n","  model.train()\n","  loss_train = AverageMeter()\n","  acc_train = Accuracy(task='MULTICLASS', num_classes=num_cls).to(device)\n","  with tqdm(train_loader, unit=\"batch\") as tepoch:\n","    for inputs, targets in tepoch:\n","      if epoch is not None:\n","        tepoch.set_description(f\"Epoch {epoch}\")\n","      inputs = inputs.to(device)\n","      targets = targets.to(device)\n","\n","      outputs = model(inputs)\n","\n","      loss = loss_fn(outputs, targets)\n","\n","      loss.backward()\n","\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","      loss_train.update(loss.item())\n","      acc_train(outputs, targets.int())\n","      tepoch.set_postfix(loss=loss_train.avg,\n","                         accuracy=100.*acc_train.compute().item())\n","  return model, loss_train.avg, acc_train.compute().item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEoGrXwZAaym"},"outputs":[],"source":["def validation(model, test_loader, loss_fn):\n","  model.eval()\n","  with torch.no_grad():\n","    loss_valid = AverageMeter()\n","    acc_valid = Accuracy(task='MULTICLASS', num_classes=num_cls).to(device)\n","    for i, (inputs, targets) in enumerate(test_loader):\n","      inputs = inputs.to(device)\n","      targets = targets.to(device)\n","\n","      outputs = model(inputs)\n","      loss = loss_fn(outputs, targets)\n","\n","      loss_valid.update(loss.item())\n","      acc_valid(outputs, targets.int())\n","  return loss_valid.avg, acc_valid.compute().item()"]},{"cell_type":"markdown","metadata":{"id":"9csGqnCvtFAS"},"source":["## Train"]},{"cell_type":"markdown","metadata":{"id":"lpJ3wtyctQJH"},"source":["### Step 1: check forward path\n","\n","Calculate loss for one batch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":857,"status":"ok","timestamp":1730636518302,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"},"user_tz":-210},"id":"QnE4F4GkzzaR","outputId":"397426e2-e480-44a4-93e9-5ad1a132b963"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]},{"output_type":"stream","name":"stdout","text":["tensor(1.3891, grad_fn=<NllLossBackward0>)\n"]}],"source":["model = RNNModel(nn.LSTM, 50, 128, 1, False, num_cls).to(device)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","x_batch, y_batch = next(iter(train_loader))\n","outputs = model(x_batch.to(device))\n","loss = loss_fn(outputs, y_batch.to(device))\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"BrHQCv7q7LF_"},"source":["### Step 2: check backward path\n","\n","Select 5 random batches and train the model"]},{"cell_type":"code","source":["from torchtext.data.functional import to_map_style_dataset"],"metadata":{"id":"1o_RsVNO4-5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMS7wj1jS6h5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730636048811,"user_tz":-210,"elapsed":2931,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"9c106f5c-b570-48e8-ede6-38388f536f95"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:248: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"]}],"source":["train_set_map_style = to_map_style_dataset(train_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jxz5DXoj61mg"},"outputs":[],"source":["_, mini_train_dataset = random_split(train_set_map_style,\n","                                     (len(train_set_map_style)-500, 500))\n","mini_train_loader = DataLoader(mini_train_dataset, 20, collate_fn=collate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8yz1meO_fUS"},"outputs":[],"source":["model = RNNModel(nn.LSTM, 50, 128, 1, False, num_cls).to(device)\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0AG7IWGEcdG"},"outputs":[],"source":["optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PK-P20hI3snf"},"outputs":[],"source":["num_epochs = 100\n","for epoch in range(num_epochs):\n","  model, _, _ = train_one_epoch(model, mini_train_loader, loss_fn, optimizer, epoch)"]},{"cell_type":"markdown","metadata":{"id":"BLT4w0ZfAhlJ"},"source":["### Step 3: select best lr\n","\n","Train all data for one epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7tlXlY4_Dwi"},"outputs":[],"source":["num_epochs = 1\n","for lr in [0.1, 0.01, 0.001, 0.0001]:\n","  print(f'LR={lr}')\n","  model = RNNModel(nn.LSTM, 50, 128, 1, False, num_cls).to(device)\n","  # model = torch.load('model.pt')\n","  optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-4, momentum=0.9)\n","  for epoch in range(num_epochs):\n","    model, _, _ = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch)\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"uC2GhaXfA8vC"},"source":["### Step 4: small grid (optional)\n","\n","Create a small grid based on the WD and the best LR\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIta-JKgA7-I"},"outputs":[],"source":["num_epochs = 5\n","\n","for lr in [0.05, 0.04, 0.03, 0.02, 0.01, 0.009, 0.008, 0.007, 0.006, 0.005]:\n","  for wd in [1e-4, 1e-5, 0.]:\n","    model = model().to(device)\n","    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n","    print(f'LR={lr}, WD={wd}')\n","\n","    for epoch in range(num_epochs):\n","      model, loss, _ = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"oBW2SLfEEkCp"},"source":["### Step 5: train more epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCCY2WecCyyg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730636103752,"user_tz":-210,"elapsed":22,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}},"outputId":"56000bbc-5aad-4dfd-f837-c056a79785ef"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]}],"source":["model = RNNModel(nn.LSTM, 50, 128, 1, False, num_cls).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bowjVB5yIXUP"},"outputs":[],"source":["lr = 0.5\n","wd = 1e-4\n","optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIrBCdHBIeRb"},"outputs":[],"source":["loss_train_hist = []\n","loss_valid_hist = []\n","\n","acc_train_hist = []\n","acc_valid_hist = []\n","\n","best_loss_valid = torch.inf\n","epoch_counter = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":575},"id":"CAXagB4yvtZd","outputId":"205f6d4f-6edf-4edd-b138-49632fa56d10","executionInfo":{"status":"error","timestamp":1730637702046,"user_tz":-210,"elapsed":1176824,"user":{"displayName":"Niloofar Nazemi","userId":"06023060329541608678"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 0: : 938batch [04:58,  3.14batch/s, accuracy=25.2, loss=1.39]\n"]},{"output_type":"stream","name":"stdout","text":["Model Saved!\n","Valid: Loss = 1.386, Acc = 0.2524\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: : 938batch [05:03,  3.09batch/s, accuracy=25.2, loss=1.39]\n"]},{"output_type":"stream","name":"stdout","text":["Valid: Loss = 1.386, Acc = 0.2524\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: : 938batch [05:25,  2.88batch/s, accuracy=25.2, loss=1.39]\n"]},{"output_type":"stream","name":"stdout","text":["Valid: Loss = 1.386, Acc = 0.2524\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: : 703batch [03:43,  3.14batch/s, accuracy=24.9, loss=1.39]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-2c9c130e59a9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   model, loss_train, acc_train = train_one_epoch(model,\n\u001b[0m\u001b[1;32m      6\u001b[0m                                                  \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                  \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-61-011a4c1a9811>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, loss_fn, optimizer, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["num_epochs = 15\n","\n","for epoch in range(num_epochs):\n","  # Train\n","  model, loss_train, acc_train = train_one_epoch(model,\n","                                                 train_loader,\n","                                                 loss_fn,\n","                                                 optimizer,\n","                                                 epoch)\n","  # Validation\n","  loss_valid, acc_valid = validation(model,\n","                                     test_loader,\n","                                     loss_fn)\n","\n","  loss_train_hist.append(loss_train)\n","  loss_valid_hist.append(loss_valid)\n","\n","  acc_train_hist.append(acc_train)\n","  acc_valid_hist.append(acc_valid)\n","\n","  if loss_valid < best_loss_valid:\n","    torch.save(model, f'model.pt')\n","    best_loss_valid = loss_valid\n","    print('Model Saved!')\n","\n","  print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n","  print()\n","\n","  epoch_counter += 1"]},{"cell_type":"markdown","metadata":{"id":"oK20iNRI3Xxb"},"source":["## Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYFzTsdIOkVp"},"outputs":[],"source":["plt.plot(range(epoch_counter), loss_train_hist, 'r-', label='Train')\n","plt.plot(range(epoch_counter), loss_valid_hist, 'b-', label='Validation')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('loss')\n","plt.grid(True)\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUgt-mWsOqhB"},"outputs":[],"source":["plt.plot(range(epoch_counter), acc_train_hist, 'r-', label='Train')\n","plt.plot(range(epoch_counter), acc_valid_hist, 'b-', label='Validation')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Acc')\n","plt.grid(True)\n","plt.legend()"]}]}